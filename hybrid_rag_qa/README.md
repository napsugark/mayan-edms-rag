# Hybrid RAG for Mayan EDMS

A Retrieval-Augmented Generation system that integrates with [Mayan EDMS](https://www.mayan-edms.com/). It indexes documents pushed by Mayan, enriches them with metadata and summaries, stores hybrid (sparse + dense) embeddings in Qdrant, and exposes a FastAPI server that Mayan calls for AI-assisted search.

## Features

- **Hybrid Search** — sparse (BM25 via `Qdrant/bm25`) + dense (`Snowflake/snowflake-arctic-embed-l-v2.0`) embeddings with cross-encoder reranking
- **Document Summarization** — automatic per-chunk summaries generated by the LLM
- **Metadata Enrichment** — entity, topic, keyword, and document-type extraction via LLM
- **Dual LLM Backend** — choose between **Ollama** (local / private) or **Azure OpenAI** via the `MODEL_TO_USE` env var
- **Qdrant Vector Store** — runs locally in Docker (no cloud account needed); cloud is optional
- **Mayan EDMS Integration** — FastAPI `/index` and `/query` endpoints that Mayan calls directly
- **Langfuse Observability** — full tracing stack included in Docker Compose
- **Resilience** — circuit breakers, rate limiters, retry-with-backoff, and embedding/retrieval/response caches

## Architecture

### Indexing Pipeline

```
Mayan EDMS ─POST /index─▶ FastAPI → PDF/DOCX/TXT/Image converter
  → Splitter → Boilerplate Filter → Metadata Enricher → Summarizer
  → Dense Embedder → Sparse Embedder → Qdrant
```

### Query Pipeline

```
Mayan EDMS ─POST /query─▶ FastAPI → Query Metadata Extractor
  → Dense Embedder → Sparse Embedder → Hybrid Retriever
  → Permission Filter → Reranker → Prompt Builder → LLM → Answer
```

## Requirements

- **Docker & Docker Compose** (primary deployment method)
- An existing **Mayan EDMS** instance running on the `mayan_mayan` Docker network
- Python ≥ 3.10, < 3.13 (only if running outside Docker)
- At least 8 GB RAM recommended

---

## Quick Start (Docker Compose)

This is the recommended way to run the stack. All services join the external `mayan_mayan` network so they can reach the Mayan EDMS container directly.

```bash
cd docker/

# Create your .env from the example
cp ../.env.example ../.env
# Edit ../.env — set MAYAN_API_TOKEN, MODEL_TO_USE, Azure keys, etc.

docker compose up -d --build
```

This starts:

| Service | Port (host) | Description |
|---|---|---|
| **rag** | `8001` | FastAPI RAG server (Swagger UI at `http://localhost:8001/docs`) |
| **qdrant** | `6333` / `6334` | Vector database (REST / gRPC) |
| **langfuse-web** | `3111` | Langfuse observability UI |
| plus | — | Langfuse worker, PostgreSQL, Redis, ClickHouse, MinIO |

> **Note:** This stack does not include its own Ollama instance. If you set `MODEL_TO_USE=OLLAMA`, point `OLLAMA_URL` to the Ollama service provided by the Mayan EDMS deployment on the `mayan_mayan` network. The default backend is `MODEL_TO_USE=AZURE_OPENAI`.

---

## Connecting to Mayan EDMS

The application connects to an existing Mayan EDMS container. In the default Mayan deployment the web service is named `app` on the `mayan_mayan` bridge network.

### From a container on the same network (default)

All services in `docker-compose.yml` already join `mayan_mayan`, so the Mayan API is reachable at:

```
MAYAN_URL=http://app:8000
MAYAN_API_TOKEN=your_mayan_api_token
```

### From the host (local development)

If the Mayan container publishes port 8000 to the host:

```
MAYAN_URL=http://localhost:8000
```

If the port is **not** published, the host cannot reach Mayan — run this application in a container on the same network instead.

### Authentication

The Mayan client supports two methods:

1. **API token** (preferred) — set `MAYAN_API_TOKEN` in `.env`.
2. **Username / password** — set `MAYAN_USERNAME` + `MAYAN_PASSWORD`; the client exchanges them for a token automatically.

Generate a token via the Mayan web UI (*Account → API tokens*) or the API:

```bash
curl -X POST "$MAYAN_URL/api/v4/auth/token/obtain/?format=json" \
  -d "username=YOUR_USER&password=YOUR_PASSWORD"
```

---

## Environment Variables

Copy `.env.example` (Docker) or `.env.local.example` (host development) to `.env` and fill in the values. Key variables:

| Variable | Default (Docker) | Description |
|---|---|---|
| `QDRANT_ENDPOINT` | `http://qdrant:6333` | Qdrant URL |
| `MODEL_TO_USE` | `AZURE_OPENAI` | `OLLAMA` or `AZURE_OPENAI` |
| `OLLAMA_URL` | `http://ollama:11434` | Ollama endpoint (when using Ollama) |
| `OLLAMA_MODEL` | `qwen2.5:3b` | Ollama model name |
| `AZURE_OPENAI_ENDPOINT` | — | Azure OpenAI endpoint URL |
| `AZURE_OPENAI_API_KEY` | — | Azure OpenAI key |
| `AZURE_OPENAI_DEPLOYMENT` | `gpt-4o-mini` | Azure deployment name |
| `MAYAN_URL` | `http://app:8000` | Mayan EDMS API base URL |
| `MAYAN_API_TOKEN` | — | Mayan API token |
| `WEBHOOK_SECRET` | — | Shared secret for webhook verification |
| `LANGFUSE_PUBLIC_KEY` | `pk-lf-local-dev-public` | Langfuse public key |
| `LANGFUSE_SECRET_KEY` | `sk-lf-local-dev-secret` | Langfuse secret key |
| `LANGFUSE_HOST` | `http://langfuse-web:3000` | Langfuse URL |

See `src/config.py` for the full list (Pydantic `BaseSettings` — all values can be overridden via env vars or `.env`).

---

## API Endpoints

The FastAPI server (`src/api/server.py`) exposes:

| Method | Path | Description |
|---|---|---|
| `POST` | `/index` | Index a document from Mayan (multipart: `document_id`, `document_version_id`, `allowed_users`, `file`) |
| `POST` | `/query` | AI-assisted search with permission filtering (JSON: `user_id`, `query`) |
| `GET` | `/health` | Health check (Qdrant connectivity, document count) |
| `GET` | `/status` | Detailed status (collection name, feature flags) |
| `GET` | `/docs` | Swagger UI |

Mayan pushes documents to `/index` with user-permission metadata. When a user searches, Mayan calls `/query` and the RAG filters results so only documents the user has access to are returned.

Supported file types for `/index`: PDF, DOCX, TXT, PNG, JPG, JPEG, TIFF, BMP.
For image files the server fetches OCR content from Mayan's API.

---

## CLI Scripts

Run with Poetry (`poetry run …`) or directly (`python …`):

| Script | Poetry shortcut | Description |
|---|---|---|
| `scripts/run_api_server.py` | `poetry run api-server` | Start the FastAPI server (dev mode) |
| `scripts/create_payload_indexes.py` | — | Create Qdrant payload indexes for metadata filtering |
| `scripts/recreate_collection.py` | — | Drop and recreate the Qdrant collection |
| `evaluation/run_evaluation.py` | — | Run the automated evaluation suite |

> `scripts/index_documents.py` and `scripts/query_app.py` still exist for legacy / debug use but require a local `data/documents_ro/` folder and a directly reachable LLM. They are not part of the normal workflow.

---

## Local Development Setup (without Docker)

You can run the FastAPI server on the host while the other services (Qdrant, Langfuse, Mayan + Ollama) remain in Docker. This is useful for rapid code iteration with auto-reload.

1. **Install Poetry** and dependencies:

```bash
pip install poetry
poetry install
poetry shell
```

2. **Configure** `.env` (copy from `.env.local.example`):

```bash
cp .env.local.example .env
```

Point the env vars at the published Docker ports:

```dotenv
QDRANT_ENDPOINT=http://localhost:6333
MAYAN_URL=http://localhost:8000      # if Mayan publishes port 8000
MODEL_TO_USE=AZURE_OPENAI            # or OLLAMA if Mayan's Ollama port is published
OLLAMA_URL=http://localhost:11434     # Mayan's Ollama, published to host
```

> **Ollama:** this project does not ship its own Ollama. When using `MODEL_TO_USE=OLLAMA`, connect to the Ollama instance provided by the Mayan EDMS deployment (either via the Docker network or a published host port).

3. **Run the API server** (with auto-reload):

```bash
poetry run api-server
# or
poetry run python scripts/run_api_server.py --reload
```

4. **Run tests**:

```bash
poetry run pytest tests/
```

---

## Configuration

All settings live in `src/config.py` as a Pydantic `BaseSettings` class. Every field can be overridden via environment variables or `.env`. Key categories:

- **LLM selection** — `MODEL_TO_USE`, Ollama settings, Azure OpenAI settings
- **Embedding models** — dense (`Snowflake/snowflake-arctic-embed-l-v2.0`), sparse (`Qdrant/bm25`)
- **Document processing** — chunk size/overlap, semantic chunking, boilerplate filtering, document-type detection
- **Retrieval** — `TOP_K`, reranker model, hybrid search weights
- **Metadata enrichment** — fields to extract (company, client, date, amount, entities, etc.)
- **Summarization** — which section types get summaries, max length, style
- **Resilience** — circuit breaker thresholds, rate limits, retry config
- **Caching** — embedding / retrieval / response cache sizes and TTLs
- **Langfuse** — keys and host for observability

### LLM Prompt Templates

Stored in `prompts/` as numbered text files:

| File | Purpose |
|---|---|
| `01_boilerplate_detection.txt` | Detect boilerplate sections |
| `02_metadata_extraction.txt` | Extract structured metadata |
| `03_summarization.txt` | Generate chunk summaries |
| `04_query_extraction.txt` | Extract filters from user queries |
| `05_rag_system.txt` | RAG system prompt |
| `06_rag_user.txt` | RAG user prompt (default) |
| `07_rag_user_concise.txt` | RAG user prompt (concise variant, currently active) |
| `08_rag_user_structured.txt` | RAG user prompt (structured variant) |

---

## Project Structure

```
hybrid_rag_qa/
├── README.md
├── pyproject.toml                  # Poetry config & script shortcuts
├── requirements.txt
├── .env.example                    # Docker env template
├── .env.local.example              # Local-dev env template
│
├── docker/
│   ├── docker-compose.yml          # Full stack (rag, qdrant, langfuse, …)
│   ├── Dockerfile                  # Multi-stage build for the RAG container
│   └── ollama-entrypoint.sh
│
├── src/                            # Core application code
│   ├── app.py                      # HybridRAGApplication facade
│   ├── config.py                   # Pydantic BaseSettings configuration
│   ├── cache.py                    # Embedding / retrieval / response caches
│   ├── resilience.py               # Circuit breakers, rate limiters, retries
│   ├── langfuse_tracker.py         # Langfuse tracing setup
│   ├── utils.py                    # Utility functions
│   ├── api/
│   │   └── server.py               # FastAPI server (/index, /query, /health)
│   ├── components/                 # Custom Haystack components
│   │   ├── boilerplate_filter.py
│   │   ├── document_type_detector.py
│   │   ├── metadata_enricher.py
│   │   ├── query_metadata_extractor.py
│   │   ├── semantic_chunker.py
│   │   └── summarizer.py
│   ├── integrations/
│   │   └── mayan_client.py         # Mayan EDMS REST API client
│   └── pipelines/
│       ├── indexing.py              # Indexing pipeline
│       └── retrieval.py            # Retrieval pipeline
│
├── scripts/                        # CLI entry points & utilities
│   ├── run_api_server.py           # Start FastAPI server (dev mode)
│   ├── create_payload_indexes.py   # Create Qdrant payload indexes
│   ├── recreate_collection.py      # Drop & recreate Qdrant collection
│   ├── index_documents.py          # (legacy) Index local folder
│   └── query_app.py                # (legacy) Interactive query CLI
│
├── prompts/                        # LLM prompt templates (01–08)
│
├── evaluation/                     # Evaluation framework
│   ├── evaluation_dataset.py
│   ├── run_evaluation.py
│   └── results/                    # JSON results from evaluation runs
│
├── tests/                          # Pytest test suite
│   ├── conftest.py
│   ├── test_config.py
│   ├── test_indexing.py
│   └── test_retrieval.py
│
└── logs/                           # Runtime logs (created automatically)
```

---

## Hybrid Search Explained

Combines two retrieval methods:

1. **Sparse embeddings** (`Qdrant/bm25`) — keyword / term matching
2. **Dense embeddings** (`Snowflake/snowflake-arctic-embed-l-v2.0`) — semantic similarity

Results are fused and re-scored by a cross-encoder reranker (`cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`).

- More sparse weight → better for exact keyword matches
- More dense weight → better for conceptual / semantic queries

---

## Performance Tips

1. **GPU acceleration** — set `EMBEDDING_DEVICE=cuda` in `.env` if you have a GPU
2. **Batch sizes** — increase `INDEXING_BATCH_SIZE` / `QUERY_BATCH_SIZE` for throughput
3. **Ollama** — use quantized models (e.g. `llama3.1:8b-q4_0`) for faster inference
4. **Caching** — embedding and retrieval caches are enabled by default; tune TTLs in config

## Troubleshooting

### Qdrant connection

```bash
# From host (if port published)
curl http://localhost:6333/collections

# From inside the Docker network
docker exec rag curl http://qdrant:6333/collections
```

### Ollama connection (when using Ollama)

```bash
curl http://localhost:11434/api/tags
```

### Mayan connectivity

```bash
# From the rag container
docker exec rag curl http://app:8000/api/v4/
```

If you get connection errors, verify that:
- The `mayan_mayan` network exists (`docker network ls`)
- The Mayan web service is named `app` on that network (`docker network inspect mayan_mayan`)
- Your `MAYAN_API_TOKEN` is valid

### Out of memory

- Reduce `INDEXING_BATCH_SIZE` in `.env`
- Use a smaller embedding model
- Process documents in smaller batches

## License

MIT License — see main repository for details
